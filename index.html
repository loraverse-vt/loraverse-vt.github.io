<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>LoRAverse</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Social preview (nice link cards) -->
  <meta property="og:title" content="LoRAverse">
  <meta property="og:description" content="A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models">
  <meta property="og:image" content="images/teaser.jpg">
  <link rel="stylesheet" href="assets/style.css">
</head>

<body>
  <header class="container">
    <h1>LoRAverse: <span class="sub">Learning XYZ from ABC</span></h1>
    <p class="authors">
      Mert Sonmezer · Matthew Zheng · Pinar Yanardag
    </p>
    <p class="links">
      <a href="#abstract">Abstract</a> ·
      <a href="#method">Method</a> ·
      <a href="#results">Results</a> ·
      <a href="#bibtex">BibTeX</a> ·
      <a href="https://arxiv.org/abs/xxxx.xxxxx">Paper</a> ·
      <a href="https://github.com/mertsonmezer/loraverse">Code</a> ·
    </p>
    <img class="teaser" src="images/teaser.jpg" alt="Teaser figure">
    <p class="caption">One short teaser caption like DIY-SC's teaser line.</p>
  </header>

  <main class="container">
    <section id="abstract">
      <h2>Abstract</h2>
      <p>Low-rank Adaptation (LoRA) models have revolutionized the personalization of pre-trained diffusion models by enabling
      fine-tuning through low-rank, factorized weight matrices specifically optimized for attention layers. These models
      facilitate the generation of highly customized content across a variety of objects, individuals, and artistic styles
      without the need for extensive retraining. Despite the availability of over 100K LoRA adapters on platforms like
      Civit.ai, users often face challenges in navigating, selecting, and effectively utilizing the most suitable adapters due
      to their sheer volume, diversity, and lack of structured organization. This paper addresses the problem of selecting the
      most relevant and diverse LoRA models from this vast database by framing the task as a combinatorial optimization
      problem and proposing a novel submodular framework. Our quantitative and qualitative experiments demonstrate that our
      method generates diverse outputs across a wide range of domains.</p>
    </section>

    <section id="method">
      <h2>Method</h2>
      <img src="images/method.png" alt="Method diagram" class="figure">
      <p>Brief, high-level method description (1–3 short paragraphs).</p>
    </section>

    <section id="results">
      <h2>Results</h2>
      <img src="images/results_qualitative.png" alt="Qualitative results" class="figure">
      <img src="images/results_quantitative.png" alt="Quantitative results" class="figure">
      <p>1–2 sentences summarizing the improvements (like DIY-SC does).</p>
    </section>

    <section id="bibtex">
      <h2>BibTeX</h2>
      <pre>@article{yourkey2025loraverse,
  title={LORAVERSE: ...},
  author={...},
  journal={...},
  year={2025}
}</pre>
    </section>

    <section id="ack">
      <h2>Acknowledgements</h2>
      <p>Grants, labs, and credits.</p>
    </section>
  </main>

  <footer class="container">
    <p>© 2025 Your Lab / Your Name</p>
  </footer>
</body>

</html>